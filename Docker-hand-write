
DOCKER TUTORIAL >>>
---------------------
24/12/22 - AshokIT

Day1

-Collection of software programs is called software project
-For any application we need 1 front end,2 back end and 3 batabase
-To run application we need code,java jdk11,webserver,springboot,mysql
-we are running our code in diff env like dev,sit(system integrate testing),UAT(user application test),pilot(pre-prod)
production
-If u wanna deploy and run the application in diff env, we need to install java,dependencies,springboot,mysql,web servers
-but practically it is very difcult and time consuming process
-so we need a proper solution for this problem to make ,it must be like a box and put togather all thse in one place
so that we can run easly in any env
-we can't deploy  our application directly to production env
- environment dependencies issues acurred when u deploying to diff env

-DOCKER has the solution for this envrironmental dependecies isses ,
-It is making the package to run the application to diff env so easily
-docker is making a box and putting all togeather in that ,that box is called container..
-hypervisor is a platform ,on top of that we can install n number of  guest OS's  1 red hat,2 ubuntu,3centos
-hypervisor is sitting on host operating system
-Containers no need of hypervizor and guest OS,docker has docker engine it will build container
for our application
-virtualizaion need hypervisor on top of that we need to install guest OS and then
 deploy application and dependenies

CONTAINERIZATION>>>>
----------------------
-Containerization is a process where we can build a package to run our application with
application code ,softwares versions,libraries,modules,os and dependecies
-container is taking care of everythig to run our application
-we can run our application in multiple environmets and diff machines
-docker is containerization software ,easly shipable
-Using docker we are going to create containers
-your complete project as a container
-docker container will taking care of software dependecies and modules and libraries
-


DAY2:>
------
--for each application we need to install one os if 3 app 3 os
-containers are very light weight,and booting time is very fast,better utilization
-size ,startup,integration VM VS CON elephant RAT,thabelu cheetah,headeche smile
-Docker is packaging ,deploying and running our application
-
-docker file:>>
-what r the dependencies ,softwares and versions required we will mention in docker file
-eg:java,web server,databse
-docker file is nothing but info about application
-docker file is information whcih required to run the APPLICATION OR PROJECT
-if u build the docker file ,docker image will created
-when docker image created ,we gonna create docker container
-
DOCKER FILE >> DOCKER IAMGE >> DOCKER CONTAINER >>
------------------------------------------------
-DCOKER file is a configuration service to build the d image
-docker hub is remote place where we can store our images in the docker hub
-docker registry is a repository
-docker cleint is a building ,pulling and running the docker cleint
-

LAB>>>>
- INSTALL ec2 instance as docker-server
-open terminal install docker
-$sudo yum install docker -y
-sudo service docker start
-docker info >but permission denied ,how to resolve?
-add docker group to ec2 user
-$sudo usermod -aG docker ec2-user
-now  restart come back to ec2-user $cnt+d then relogin and see docker info now u r granted
- to know the docker images $docker images or docker image ls
-to know the docker container $docker container ps
-docker -v > which version?
-dcoker hub to pull and push the images
-
-docker pull image name or id
-dcoker login to login docker hub
-docker pull or run imagesID
-dcoker rmi <images name or id >to delete
-to run docker images $docker run images id
-

Day3..>>>>
-Docker file contains instructions to create image by using some speacial keywords
-dcoker images is package whcih is everything to run the application like
code,software ,versions,modules,dependencies,env .nothing but full pack
-Docker container is runtime instance of docker image.our application run or execute only on container.
-Docker registry is remote rep to store our images
-DOCKER ENGINE is a software where we create images and containers
-Nexus,jfrog,aws ecr,dtr are private registry to store the images we build
-
----when docker server connecting use theese commands
--------------------------->>>>>>>>>>>>>>>>>>>>>>>>>
-docker -v,docker info,docker ser

-sudo service docker start >>>to start the service
-to delete the image $docker rmi dID -f
-u can create an image even without mentioning pull command
-$docker run mysql
-first it will check in local if unable to find then going tp library and pulling
-

How to create our own docker images>>>>
------------->>>>>>>>>>>>>>>>>>>>>>>.>>>>>>>>>>>>>>
-first we need to login to our ec2 instance by login credentials of docker hub
-let's create docker file to create image
-sudo vi Dockerfile
-FROM ubuntu,RUN echo "hello",RUN echo "arun",CMD echo "images"
-Now create an image from docker file $dokcer build -t
tag-name . (dot is searching for the file in urrent directory
-

-now push the image to docker public hub
-first we need to tag to that image
-$docker tag myfirst-img arunponugotii/myfirst-img
-$docker push arunponugotii/myfirst-img #now see in docker hub,yes done..
-now all diff envs can pull this image and excute it within secounds ,diff machines around the world
-

-DOCKER IMAGES IS A FULL PACK WHEREEVERYTHING IS AVAILABE TO EXECUTE THE APPLICATION.
-Now u don't need to install java,tomcat,dependencies to run the application u already configured in dockerfile
-
-

>>>DOCKER FILE>>>>>>
-------------------
-Docker file set of keyword to build the image
-In docker file we use DSL(domain specific language )keywords
- docker engine proccess the dockerfile instructions from top to bottom
-FROM,RUN,MAINTAIN,CMD,VOLUME,COPY,ADD,ENTRYPOINT,ENV,LABEL,USER,WORKDIR,EXPOSE,
-FROM ,MAINTAINER,ADD,COPY,CMD,RUN,ENTYPOINT,ENV,LABEL,EXPOSE,VOLUME,USER,WORKDIR
-FROM,MAINTAINER,COPY,ADD,CMD,RUN,ENTRYPOINT,ENV,LABEL,USER,WORKDIR,EXPOSE,VOLUME

-FROM keyword:pulling the base image and on top of that we will create our own image
+++++
sytax : FROM <image-name> FROM ubuntu,mysql,tomcat:9.2,java:jdk-1.8.0

-MAINTAINER:who is the author of this file arun <arun.gmail.com>
+++++++++++
COPY :to cpoy the file or folder
++++
sytax: COPY <source> <destination> .war file is stores in target file but we need mv .war file to webapp file 
to run the application in tomcat server COPY target/mvn-web-app.war  /user/local/tomcat/webapp/mvn-web-app.war

-ADD:add keyword also used to copy the file ,but it is from brower(http)remote location to dockerfile
++++
if u add file to copy a remote file it will automatically extract the tar file,not zip files to unzip
syntax:ADD <http file > <destination>
-
RUN:it is used to execute commands on top of base image ??  
++++
Run command executes when u creating the images
RUN is a executable command like creating a file ,RUN mkdir workspace-file,RUN yum install git

CMD:It is also executable command ,but for container,CMD command execute while containers are creating
++++

-git install on top of image ,but tomcat shoild execute only while creating container (later)
====git=image,,tomcat =====container
-container execution means application execution
-multiple RUN commands will execute ,but multiple CMD commands won't 
-last command will only execute
- 
-to build images from docker file :
$docker build -t imageone
-to run the image :
$docker run imageone #we can override the CMD command by using runtime CMD

$DOCKER RUN imageone date
#it will print only date(CMD not executing)

-how to change dockerfile?
$mv Dockerfile 	Dockerfilew-one 
-create image after changing the name
$docker build -f Dockerfile-one -t imagetwo .(-f must)

ENTRYPOINT:it will execute while creating container,we can't override it
+++++++++++

WORKDIR:WORKING DIRECTORY
+++++++++
-where we want to launch our application ,we can specify
sytax: WORKDIR <dir-path>
-all dockerfile instructions which are running in WORKDIR ,after u specify
-Whatever command u given in dockerfile they are going to execute after WORKDIR
-It is used while working with tomcat,start.sh whcih is in bin directory so u have to change the configure
-
ENV:Is used to set the invironmet variables ENV <KEY> <VALUE>
+++

LABEL: IT IS USED TO ADD META DATA FOR OUR IMAGE
+++++
 -label will represent key value pair

ARG:it is used to avoid hard coded values in dockerfile
++++

USER:we can set user for image/container
+++++
Note:


EXPOSE:about ports and rules for container running
+++++

VOLUME:it is used for storing the data
+++++

Container
+++++++++:Create a container with docker images which was created by dockerfile name is called springboot
-$docker run -d -p 8080:8080 con-name


DAY5: How to build .war file
-create a folder with name ,navigate to that folder
-clone the code and $mvn clean package
-now it will biuld the code
-now create an image with that .war file and run container that's it
-if java application devloped by springboot u dont need .war file,if not u need .war file ??
-to run container:>>> $docker run -d -p 9000:8080 #9090:8080 image-name .
        +++++++++


-PROJECT -2
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-python-fask-application
+++++++++++++++++++++++++++
-DOCKER FILE
https://github.com/ashokitschool/python-flask-docker-app.git
FROM python:3.6

MAINTAINER Ashok Bollepalli "ashokitschool@gmail.com"

COPY . /app

WORKDIR /app

EXPOSE 5000

RUN pip install -r requirements.txt

ENTRYPOINT ["python", "app.py"]

-expose 5000 is container code ,u can't change ,but u can change host port number
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


-to delete all the containers $docker system prune -a #STOPED AND UNUSED IMAGES
-$docker ps -a all show
-$docker stop continer ID
-$docker rm con ID # TO REMOVE
-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++
DOCKER VOLUMES:
+++++++++++++++++++++++++++>>>>
-To store the data
--If a container creates some data which is stored in database and if it is killed or terminated automatically
that data will lost,=
-that's why to solve this DOCKER VOLUMES came into the picture ,
-docker volumes helps us to share our data from con to con
-why do we need to share our data from one con to anothe con ??
-docker volumes stores in docker host machine (aws ec2 or lambda)
-docker volume:>> Container generated data it will stores and sending it to the other docker containers also
-Even u  deleted the docker container ,docker container won't delete
-backup data and sharing
-where will be we store our docker volumes in aws
-
+++++++++++++++++++++++++++++++++++
MICROSERVICE BASED APPLICATIONS :-
++++++++++++++++++++++++++++++++++++++
-EX:Product purchased container is gererating the container
-in one docker volumes 
-application tracking system need to get data from product purchased con to track 
-For thatdocker volume database will help us to share from produt purch cont to applic track contai
-
+++++++
docker commands
+++++++++++++++++++++
-docker volume --help #to know more about commands
-we have mainly 5 commands in docker volumes
-1 create #docker volumes 	will cretes
-2 inspect # display all the info about volumes
-3 ls #to list out the volumes
-4 prune #to delete all unused volumes
-5 rm #to dlete one or more volumes
-
- docker run --name=web-app -d -p 8080:80 nginx
c7b85805b5a19f3746f2bf7f3eadd092a7ac6586d1e7e4cd92fdce5609d6014b
--web-app is my container name
-to change the content inside of container >>>>>
-docker exec -it web-app bash #opening to in bash 
-it will go to container machine from host machine here u can write anything which will belongs to container
-root@c7b85805b5a1:/# #it is container machine
-cd /user/share/nginx/html
-u gonna see index.html
-echo "hello arun welcome to nginx" > index.html
-represh the webpage of ip adress
- but if u stop or terminate this container then data will be gone ,if u create new container it will not retrive 
-so we need to store data for aother container
-so we need to attach docker container to docker volume 
-$docker volume create new-vol
-to mount docker container with docker volume
-$docker run -d --name=web-app2 --mount source=new-vol,destination=/user/share/nginx/html -p 8080:80 nginx
-change some content in new docker container
-$docker exec -it we-app2 bash 
-$cd /user/share/nginx/html
-echo "hello arun for docker volume" > index.html
-now stop the this container and create anethor container
-$docker run --name=web-app3 --mount source=new-vol,destination=/usr/share/nginx/html -p 8080:80 -p nginx

+++++++++
DAY-6
++++++++++++++++

-MOUNTING TO DIRECTORY 
-it is same as docker volume but there is small diff ,if u wanna store data in directory it is usefull,s
   storing data in local machine
-$mkdir -p /tmp/nginx/html
-navigate to that directory
-docker run -t -d -p 80:80 -v /tmp/nginx/html:/usr/share/nginx/html --name c1 nginx:latest
-docker container ls
-create index.html > whatever data u created in local it will shows in web server,what ever content container
  created it will shows in local directory
- docker container is by defualt stateless container #data will delete if container delete,
-statfullness we have to do with docker volume nad binded mount (directory)


+++++++++
- what is virtualization,containarization,what is docker ,advantages,files,images,containers,volumes,mounting,
dockerr terminlogy,commands,bind mount,spring boot docker file ,


+++++++++++++++++++++++
++++++++++++++++
Monolith  vs Microservise
+++++++++++++++++++++++++
-90% containers are microservice based containers
-Monolith means we are creating only one container for our application to run our app
-but microservices we need to run lot of containers to run app
-



+++++++++++
Docker compose
++++++++++++++
+++++++++++++++++
-it is a tool to manage multi container based applications
-Now a days we are devloping applications using microservices with the help of API's
-every api is a saperate project
-Running multiple containers manually is defcult task .
-Using docker compose we can deploy multi containers to run our applications
-out project need 100 containers to run at a time order by order docker compose make it possible 
-
<<<====>100 API>>>>>===>>100 Microservices>>====>100 Containers>>====>Docker compose>>
-docker compose running containers using YAML scripts
-
Sample docker composeYAML:
version:
services:
network:
volumes:
 
docker compose file name:docker-compose.yml
#to create and start the container
$docker-compose up
$docker compose ps $to know how many containers 
$docker compose down #to stop and remove
$docker compose images # images ls
$docker -compose -f <fname> up

+++++++++++++
docker -compose install
+++++++++++++++++++++++
Docker CE Install
sudo amazon-linux-extras install docker
sudo service docker start
sudo usermod -a -G docker ec2-user
Make docker auto-start

sudo chkconfig docker on

Because you always need it....

sudo yum install -y git

Reboot to verify it all loads fine on its own.

sudo reboot

docker-compose install
Copy the appropriate docker-compose binary from GitHub:

sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose

NOTE: to get the latest version (thanks @spodnet): sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose

Fix permissions after download:

sudo chmod +x /usr/local/bin/docker-compose

Verify success:

docker-compose version

++++++++++++++++++++++++++++++++++++++++++++++
-now create a docker compose project by using mysql and wordpress and attach them to store the data from wordpress
-Docker-compose is linking the two diff containers like product cont link with db container,and tracking con
also linking with db cont,cart con also link with db container
-
-#for installing  mysql by using docker-compose
$https://medium.com/@chrischuck35/how-to-create-a-mysql-instance-with-docker-compose-1598f3cc1bee



########################
DOCKER Networking:
#####################
-Docker networking is about communication between two containers
-docker networking is providing security and isolation to containers
-docker networking is providing single operation system to many containers in a isoalation manner
-it helps to delever the service very fast
-Aplication portability can happned by using docker networking
-when u install docker 3 default networks configring 
-1 none ,2 host ,3 bridge
-We can configure one container to many networks to communication purpose
-Docker is providing networks to containers by using docker network divers

+++++++++++
-Network drivers
++++++++++++++++++++==
-Bridge 2 host 3 none 4 overlay 5 Macvlan
-Bridge driver:this is default driver for docker host machine,
-It is very helpfull when u r running standalone container(single) 
$docker network inspect bridge #more detailes
-
Host driver:
+++++++
-it is very helpfull when standalone container available #not defualt
-The container will not get any IP adress when u enable Host driver
-it is comming with 80 port number ,in this case we dont need to map  many containers to one port number
to host machine port

-The advantage of this network driver is when u r using many containersn\

-Non driver:no network to acces to container

-overlay container:
++++++++++++++++-

-we use overlay network driver for docker swarm
-
macvlan driver:Assigning MAC address  to containers
-it is simplifing the communication between two containers
-

++++++++++++
Docker commands
+++++++++++++
-docker network ls
-docker network inspect bridge #more info

-We can create our own bridge network
-$docker network create --driver bridge my-bridge-network
-$docker network create --driver bridge my-network-bridge 

-#Run the container using custom network <my-network-bridge>
-$docker run -d --name my-nginx --network my-network-bridge -p 8080:80 nginx
-Now container created and attached with my custom network my-network-bridge
-#To see $docker network inspect my-network-bridge

#to delete all running containers
$docker rm -f $(docker ps -aq)

#now creating two cotainers and attaching one network to them and tesing
-$docker run --name nginx30 -d --network my-network-bridge -p 8080:80 nginx
-$docker run --name nginx31 -d --network my-network-bridge -p 9000:80 nginx
-For each container having one IP address
-how to get that ip adress
-$docker inspect -f '{{range.NetworkSettings.Networks}} {{.IPADDress}}{{end}}'nginx30 
-$docker inspect -f '{{range.NetworkSettings.Networks}} {{.IPAddress}}{{end}}' nginx31

- #now try to ping one container to another containerr with IPadress
-docker exec -it nginx30 /bin/bash
-root$ apt-get update
-$apt-get install iputils-ping
$now $ ping 172.19.0.3 do same to another container
$

########
Running container with host network driver
+++++++++++++++++++++++++++++++++++++
- port mapping is not requierd ??



################
DOCKER SWARM:-
+++++++++++++++++
-Docker swarm is a Orchestration tool for running containers
-Handling the failovers,managing the process of contaiers like load balancing,Auto scaling
-Docker swarm is setting up docker cluster
-What is cluster ? Running our application in multiple nodes or server is called cluster or container
-In the cluster Architecture we have one master node nd 2+ slave nodes
-Master nodes is managing and executing the application of slave nodes
-slave nodes are nothing but application servers 
-Master node sceduling and assign the work to slave nodes
-we r not running our application in master nodes ,to run our app we use slave nodes
-If one slave node is failed master node will create another node to run our application,
-But if our master node is failed entire system collapsed
-For better performance of master Orchestrtion we have a formula called Quarm (n-1)/2
-if u created two master 2-1/2= .5,not even single machine
-so preffer three master nodes 3-2/2=1 # 1 master is highly available even one master node is failed
-
-+++++++++++++
Install Docker swarm in 3 machines
++++++++++++++++++++++++++++++++++
-$curl -fsSL https://get.docker.com -o get-docker.sh
-$sudo sh get-docker.sh
- $sudo docker info
#docker swarm is inactive

-How to make a normal node to be master node?
-When u run a command called init that will become master node
-sudo docker swarm init -advertise-addr Private-IP of current machine to whom u attach
-Now it is a manager of all the docker slave nodes
-If u wanna more managers for haigh available below cammad run with sudo u gonna get 
-docker swarm join-token manager #after u created one manager node then it shows
-if u wanna add more nodes for ur manager then run the command
-$docker swarm join-token worker #after 
-Now to to docker info and see swarm will be active and manager will be true
-

===========================
Deploy applicationas a service  using docker swarm
=================================
-Docker swarm service
-if i use docker run command to create container it will only create in one container
-but we need our application in multiple server nodes
-
-docker swarm service is colletion of many containers of same image
-there are two types of swarm services -1 Replica 2 Global

-To create a container for cluster

-$sudo docker service create --name java-web-app -p 8080:8080 ashokit/javawebapp
-Now onwards u have to check like docker service
-docker service ls #for containers
-3.108.64.161:8080/java-web-app/ #publicIP brows it was done succesfully
-Replica means how many nodes are running
-by defualt one replica gonna created
-Check how many nodes are running my Application
-$sudo docker service ls ,here u see replica how many nodes are running
-To inspect how many replicas are running
-$sudo docker service inspect ps java-web-app #container name
-If u wanted one docker remove
-$sudo docker swarm leave
now check 
-$sudo docker service inspect ps java-web-app   























 
























































 









